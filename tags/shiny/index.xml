<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shiny on Coding with Data</title>
    <link>/tags/shiny/index.xml</link>
    <description>Recent content in Shiny on Coding with Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/shiny/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Lightweight streaming analytics with NATS</title>
      <link>/blog/lightweight-streaming-analytics-with-nats/</link>
      <pubDate>Tue, 02 Oct 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/lightweight-streaming-analytics-with-nats/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

library(knitr)

eng_go &lt;- function(options) {

  # create a temporary file

  f &lt;- basename(tempfile(&#34;go&#34;, &#39;.&#39;, paste(&#39;.&#39;, &#34;go&#34;, sep = &#39;&#39;)))
  on.exit(unlink(f)) # cleanup temp file on function exit
  writeLines(options$code, f)

  out &lt;- &#39;&#39;

  # if eval != FALSE compile/run the code, preserving output

  if (options$eval) {
    out &lt;- system(sprintf(&#39;go run %s&#39;, paste(f, options$engine.opts)), intern=TRUE)
  }

  # spit back stuff to the user

  engine_output(options, options$code, out)
}

knitr::knit_engines$set(go=eng_go)
&lt;/style&gt;
&lt;div id=&#34;go-in-the-fast-lane&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Go in the fast lane&lt;/h2&gt;
&lt;p&gt;Fast data is the new big data. But how difficult is it really to set up a complete streaming analytics solution from the ground up? It turns out not that hard, not if you are using &lt;a href=&#34;https://github.com/nats-io/go-nats-streaming&#34;&gt;NATS Streaming&lt;/a&gt;. Developed in Go&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚Äú‚Ä¶NATS Streaming is an extremely performant, lightweight reliable streaming platform built on NATS.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have been wanting to experiment with Go for some time, so building a toy service on top NATS Streaming seemed like a interesting project to start with. To get familiar with the basics, I worked through about two thirds of the &lt;a href=&#34;https://github.com/nats-io/go-nats-streaming&#34;&gt;Tour of Go&lt;/a&gt; - a dope interactive introduction to the fundamentals of the language. At this point, I figured I know enough for the fifty or so lines of Go code I was about to write and headed for the IDE. What I had in mind was the classic streaming analytics demo: The real-time Twitter dashboard. After some initial research I was able to break down the task at hand into 4 subtasks:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Communicate with the &lt;a href=&#34;https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data.html&#34;&gt;Twitter Streaming API&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Ingest tweets into &lt;a href=&#34;https://github.com/nats-io/go-nats-streaming&#34;&gt;NATS Streaming&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Provision a MySQL database where the tweets will be written to.&lt;/li&gt;
&lt;li&gt;Create a Shiny App as a (near) real-time NLP dashboard.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To ensure everything is working as expected I‚Äôll use Docker containers in conjunction with &lt;a href=&#34;https://docs.docker.com/compose/overview/&#34;&gt;Docker Compose&lt;/a&gt; as the orchestration tool.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-nats-streaming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About NATS Streaming&lt;/h2&gt;
&lt;p&gt;Undeniably, Kafka is the most widely used streaming solution right now. But is it the only option out there? Is it even the best option? It depends, of course. But if you prefer a lightweight footprint and simplicity without sacrificing performance, NATS is very, very hard to beat. NATS Streaming is a service layer on top the original NATS framework. The latter was originally conceived as a distributed messaging system with few guarantees, but blazing fast performance. NATS Streaming extends the original framework through the introduction of at-least-once delivery, durable storage, message replay and a couple other enhanced quality of service features.&lt;/p&gt;
&lt;p&gt;The central piece is the NATS (Streaming) Server. It manages subscriptions on specific subjects and handles communications between clients. Once the server is up and running, we can create and publish messages unto subjects, and on the receiving end subscribe to them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/nats.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nats-io/nats-streaming-server#getting-started&#34;&gt;Installing the package&lt;/a&gt; creates an executable &lt;code&gt;nats-streaming-server.go&lt;/code&gt; that we can run to start the server.&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;[12505] 2018/10/01 11:53:13.037897 [INF] STREAM: Starting nats-streaming-server[test-cluster] version 0.11.0
[12505] 2018/10/01 11:53:13.038015 [INF] STREAM: ServerID: iDV733mTgjWieVayqCLiG2
[12505] 2018/10/01 11:53:13.038022 [INF] STREAM: Go version: go1.11
[12505] 2018/10/01 11:53:13.038880 [INF] Starting nats-server version 1.3.0
[12505] 2018/10/01 11:53:13.038894 [INF] Git commit [not set]
[12505] 2018/10/01 11:53:13.039199 [INF] Listening for client connections on 0.0.0.0:4222
[12505] 2018/10/01 11:53:13.039208 [INF] Server is ready
[12505] 2018/10/01 11:53:13.068118 [INF] STREAM: Recovering the state...
[12505] 2018/10/01 11:53:13.068165 [INF] STREAM: No recovered state
[12505] 2018/10/01 11:53:13.320178 [INF] STREAM: Message store is MEMORY
[12505] 2018/10/01 11:53:13.320295 [INF] STREAM: ---------- Store Limits ----------
[12505] 2018/10/01 11:53:13.320305 [INF] STREAM: Channels:                  100 *
[12505] 2018/10/01 11:53:13.320312 [INF] STREAM: --------- Channels Limits --------
[12505] 2018/10/01 11:53:13.320320 [INF] STREAM:   Subscriptions:          1000 *
[12505] 2018/10/01 11:53:13.320329 [INF] STREAM:   Messages     :       1000000 *
[12505] 2018/10/01 11:53:13.320337 [INF] STREAM:   Bytes        :     976.56 MB *
[12505] 2018/10/01 11:53:13.320343 [INF] STREAM:   Age          :     unlimited *
[12505] 2018/10/01 11:53:13.320349 [INF] STREAM:   Inactivity   :     unlimited *
[12505] 2018/10/01 11:53:13.320356 [INF] STREAM: ----------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, NATS Streaming persists the messages as files. It suffices to start publishing messages to a subject, and they will be saved to memory. The other option is a MySQL database. Using the flags &lt;code&gt;--store&lt;/code&gt;, &lt;code&gt;--sql_driver&lt;/code&gt; and &lt;code&gt;--sql_source&lt;/code&gt; when starting the &lt;code&gt;nats-streaming-server&lt;/code&gt;, we can configure access to the database, or alternatively supply a &lt;code&gt;.conf&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Once the server is up and running, we can create subjects and publish messages. A simple worker program written in Go that ingests data from the Twitter Streaming API, and funnels it into the database using NATS Streaming, is barely ~50 lines of code.&lt;/p&gt;
&lt;pre class=&#34;go&#34;&gt;&lt;code&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;stream/keys&amp;quot;

    &amp;quot;github.com/dghubble/go-twitter/twitter&amp;quot;
    &amp;quot;github.com/dghubble/oauth1&amp;quot;
    stan &amp;quot;github.com/nats-io/go-nats-streaming&amp;quot;
)

func main() {
    var err error
    word := os.Getenv(&amp;quot;TWITTER&amp;quot;)  // Get word to filter Twitter stream on 

    config := oauth1.NewConfig(keys.Key, keys.Secret)
    token := oauth1.NewToken(keys.Token, keys.TokenSecret)
    httpClient := config.Client(oauth1.NoContext, token)

    // Twitter client
    twitterClient := twitter.NewClient(httpClient)
    // Nats client
    natsClient, err := stan.Connect(&amp;quot;test-cluster&amp;quot;, &amp;quot;test&amp;quot;,
        stan.NatsURL(&amp;quot;nats://nats:4222&amp;quot;))
    if err != nil {
        log.Fatal(err)
    }
    
    // Convenience Demux demultiplexed stream messages
    demux := twitter.NewSwitchDemux()
    demux.Tweet = func(tweet *twitter.Tweet) {
        natsClient.Publish(word, []byte(tweet.Text))
    }

    // Filter parameters for Twitter stream
    filterParams := &amp;amp;twitter.StreamFilterParams{
        Track:         []string{word},
        StallWarnings: twitter.Bool(true),
        Language:      []string{&amp;quot;en&amp;quot;},
    }
    
    stream, err := twitterClient.Streams.Filter(filterParams)
    if err != nil {
        log.Fatal(err)
    }
    for message := range stream.Messages {
        demux.Handle(message)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That‚Äôs it. Of course this is only two pieces of the puzzle. We still need to provision a SQL database for the message store and build a Shiny App to munge and visualize the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;meet-the-architect-docker-compose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Meet the architect: Docker Compose&lt;/h2&gt;
&lt;p&gt;Instead of creating the remaining services one by one and linking them up, it is better to deploy this mini infrastructure in its entirety. With Docker containers, we can package each piece with all its dependencies. Using Docker Compose, we can configure how the containers should work in tandem and communicate with each other if needed.&lt;/p&gt;
&lt;p&gt;With a Docker image for everything nowadays, our Dockerfiles won‚Äôt be long. Adding a couple of &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/db/Dockerfile&#34;&gt;environment variables here&lt;/a&gt; or &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/shiny/Dockerfile&#34;&gt;installing additional package there&lt;/a&gt;, most configurations will already be taken care of by the read-only layers of the base images.&lt;/p&gt;
&lt;p&gt;Our infra consists of 4 containers: One for the MySQL database, the NATS streaming server, the NATS worker that will publish the messages and finally the Shiny app. A couple of pointers with regards to the &lt;code&gt;docker-compose.yml&lt;/code&gt; file below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;build: context:&lt;/code&gt; parameter is the location of the Dockerfile.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;restart: always&lt;/code&gt; is set because services aren‚Äôt booted in sequence despite dependencies.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ports:&lt;/code&gt; will be shared among services, and also exposed to the outside world.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;environment: - TWITTER=${TWITTER}&lt;/code&gt; ensures that the &lt;code&gt;$TWITTER&lt;/code&gt; environment variable (as defined in the &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/.env&#34;&gt;.env file&lt;/a&gt;) is available for all.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without further ado, this is what the final &lt;code&gt;docker-compose.yml&lt;/code&gt; file looks like:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;version: &amp;quot;3&amp;quot;

services:
  db:
    build:
      context: &amp;quot;./db&amp;quot;
    restart: always
    command: --default-authentication-plugin=mysql_native_password
    ports:
      - &amp;quot;3306&amp;quot;
  nats:
    image: nats-streaming:latest
    restart: always
    depends_on:
      - db
    command: -m 8222 --store SQL --sql_driver mysql --sql_source &amp;quot;root:pwd@tcp(db:3306)/nss_db&amp;quot;
    ports:
      - &amp;quot;4222&amp;quot;
      - &amp;quot;8222:8222&amp;quot;
  nats-worker:
    build:
      context: &amp;quot;./nats&amp;quot;
    environment:
    - TWITTER=${TWITTER}
    restart: always
    entrypoint: /go/main
    depends_on:
      - nats
  shiny:
    build:
      context: &amp;quot;./shiny&amp;quot;
    environment:
    - TWITTER=${TWITTER}
    ports:
      - &amp;quot;80:3838&amp;quot;
    depends_on:
      - db&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To build all the images we can use &lt;code&gt;docker-compose build&lt;/code&gt;; the command to spin up the services is &lt;code&gt;docker-compose -f docker-compose.yml up&lt;/code&gt;. Similarly to stop the containers we have &lt;code&gt;docker-compose stop&lt;/code&gt; and &lt;code&gt;docker-compose rm -fv&lt;/code&gt; to get rid of the stopped containers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-apps-and-streaming-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny Apps and streaming data&lt;/h2&gt;
&lt;p&gt;I have written about &lt;a href=&#34;http://tamaszilagyi.com/blog/dockerized-shiny-app-development/&#34;&gt;Shiny Apps and how to containerize them before&lt;/a&gt;, so I will only briefly touch upon dealing with real-time data here. As we have seen before, NATS Streaming is continuously dumping new data into our MySQL database according to a &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/db/dump/schema.sql&#34;&gt;predefined schema&lt;/a&gt;. On the R side it turns out we have pretty sweet tools for dealing with databases, such as &lt;a href=&#34;https://shiny.rstudio.com/articles/pool-basics.html&#34;&gt;pool and DBI&lt;/a&gt;. Specifically for shiny apps, there is also a function called &lt;code&gt;shiny::reactivePoll()&lt;/code&gt; that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚Äú..create a reactive data source, which works by periodically polling a non-reactive data source.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Two of the required arguments are functions. One to check whether some value in our database has been updated, and if so, one to pull the updated data from the database. The other two required arguments are the number of milliseconds to wait between checks, and the user session.&lt;/p&gt;
&lt;p&gt;This is the relevant bit from the shiny app:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pool)
library(DBI)
library(shiny)
library(anytime)
library(tidytext)

pool &amp;lt;- dbPool(
        drv = RMySQL::MySQL(),
        dbname = &amp;quot;nss_db&amp;quot;,
        host = &amp;quot;db&amp;quot;, 
        port = 3306,
        username = &amp;quot;nss&amp;quot;,
        password = &amp;quot;password&amp;quot; 
)

data &amp;lt;- reactivePoll(1000, session,
             # This function returns the latest timestamp from the DB
             checkFunc = function() {
                     pool %&amp;gt;% tbl(&amp;quot;Messages&amp;quot;) %&amp;gt;%
                             summarise(max_time = max(timestamp, na.rm = TRUE)) %&amp;gt;%
                             collect() %&amp;gt;%
                             unlist()
                     
             },
             # This function returns a data.frame ready for text mining
             valueFunc = function() {
                     pool %&amp;gt;% tbl(&amp;quot;Messages&amp;quot;) %&amp;gt;%
                             filter(!data %like% &amp;quot;%http%&amp;quot;) %&amp;gt;% 
                             arrange(-timestamp) %&amp;gt;%
                             head(20000) %&amp;gt;%
                             collect() %&amp;gt;%
                             mutate(data = gsub(&amp;quot;[^[:alnum:][:space:]]&amp;quot;,&amp;quot;&amp;quot;,data)) %&amp;gt;%
                             unnest_tokens(word, data) %&amp;gt;%
                             anti_join(stop_words) %&amp;gt;% 
                             mutate(timestamp = anytime(timestamp/1e+9)) %&amp;gt;%
                             inner_join(get_sentiments(&amp;quot;bing&amp;quot;)) 
                   
             }
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After establishing the pool connection, it is used to check whether the latest timestamp is different from the previous one. If that‚Äôs the case, we pull the last 20.000 tweets from the database, collect it as an R &lt;code&gt;data.frame&lt;/code&gt; and transform it using the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html&#34;&gt;tidytext&lt;/a&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Postscript&lt;/h2&gt;
&lt;p&gt;The app I have outlined in this post is currently live on &lt;a href=&#34;http://stream.tamaszilagyi.com/&#34;&gt;stream.tamaszilagyi.com&lt;/a&gt; plotting a few metrics for tweets containing the word ‚Äútrump‚Äù, for demonstration purposes. It is running on a small Linux VM on Azure so don‚Äôt be intimidated by slow load times. I only have so much free Azure credit.&lt;/p&gt;
&lt;p&gt;With minor modifications though, we could deploy our containers onto a cluster of computers and scale the crap out of this little streaming service. Such is the beauty of cloud resources and using cloud-native technologies like Docker and NATS.&lt;/p&gt;
&lt;p&gt;As always, all the code is on my &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny&#34;&gt;GitHub&lt;/a&gt;, including instructions on how to try it for yourself.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dockerized Shiny App development</title>
      <link>/blog/dockerized-shiny-app-development/</link>
      <pubDate>Tue, 16 Jan 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/dockerized-shiny-app-development/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;getting-on-the-docker-container-ship&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting on the Docker (container) ship&lt;/h2&gt;
&lt;p&gt;Containers are everywhere, including the realms of data science. You can think of them as small self-contained environments, encapsulating an application and its dependencies. If that sounds a lot like a virtual machine, you are not entirely wrong. But unlike VM‚Äôs, containers run on the host system‚Äôs kernel and the processes inside can only see and access their immediate surroundings.&lt;/p&gt;
&lt;p&gt;Thanks to the good people behind the &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;rocker project&lt;/a&gt;, there‚Äôs already plenty of R-specific Docker images available for folks looking to containerize their R code. The most often cited benefits are &lt;em&gt;portability&lt;/em&gt; and &lt;em&gt;reproducibility&lt;/em&gt; of your analysis. In the same vein, &lt;a href=&#34;https://maraaverick.rbind.io/2017/11/docker-izing-your-work-in-r/&#34;&gt;lots of great material&lt;/a&gt; is out there with respect to what these bad boys exactly are and how to get them up and running.&lt;/p&gt;
&lt;p&gt;But I haven‚Äôt found much on &lt;em&gt;Docker based workflows&lt;/em&gt;, especially how to go about developing dockerized shiny apps. Because what if I want to build a shiny dashboard inside a container, integrate it with &lt;a href=&#34;https://travis-ci.org/&#34;&gt;Travis CI&lt;/a&gt; and run tests on every single commit to GitHub?&lt;/p&gt;
&lt;p&gt;The code in this post is based on a bare bones shiny app (containing USA Trade data) I built for illustration purposes. You can find the app &lt;a href=&#34;http://usatrade.tamaszilagyi.com/&#34;&gt;here&lt;/a&gt;, and all the code on &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testable-shiny-apps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testable shiny apps&lt;/h2&gt;
&lt;p&gt;We all heard of unit testing, but can we test an actual shiny application? As often the case in the R world, &lt;em&gt;there is already a package for that:&lt;/em&gt; &lt;a href=&#34;https://github.com/rstudio/shinytest&#34;&gt;shinytest&lt;/a&gt; - an automated testing agent for, you guessed it‚Ä¶shiny apps. It works as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Shinytest uses snapshot-based testing strategy. The first time it runs a set of tests for an application, it performs some scripted interactions with the app and takes one or more snapshots of the application‚Äôs state. These snapshots are saved to disk so that future runs of the tests can compare their results to them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The interface is super easy. You install the package and when the first version of your shiny app is ready to roll, you simply run &lt;code&gt;recordTest()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/shinytest&amp;quot;)
library(shinytest)

recordTest(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This launches an iframe consisting of your dashboard and controls over what to test. Each interaction with the dashboard is recorded, and when you hit &lt;strong&gt;take snapshot&lt;/strong&gt;, the state of your dashboard is saved, along with raw scripts to reproduce the interactions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/t3xcuCX.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Upon exiting the test event recorder, a new folder &lt;code&gt;test/&lt;/code&gt; is created inside the app‚Äôs directory, containing both the test script - &lt;code&gt;dates.R&lt;/code&gt;, as well as the application‚Äôs state as a .json and a .png files in &lt;code&gt;test/dates-expected&lt;/code&gt;. The latter serve as expected output, based on which consequent runs of tests shall be evaluated. Using my example app, &lt;code&gt;dates.R&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(shiny.testmode=TRUE)

app &amp;lt;- ShinyDriver$new(&amp;quot;../&amp;quot;, seed = 123)
app$snapshotInit(&amp;quot;dates&amp;quot;)

app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, running &lt;code&gt;testApp(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt; will look for test scripts inside the &lt;code&gt;test/&lt;/code&gt; folder, and run them to recreate the state of the test recording, comparing the output to what‚Äôs expected. It is generally a good idea to only compare the .json files, because the screenshots of the app (the .png file) will likely differ of various systems. We pass the argument &lt;code&gt;compareImages = FALSE&lt;/code&gt; to bypass default behavior. A full fledged test script will then look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(testthat)
test_that(&amp;quot;Application works&amp;quot;, {
        expect_pass(testApp(&amp;quot;/srv/shiny-server/myapp/&amp;quot;,
                            testnames = &amp;quot;dates&amp;quot;,
                            compareImages = FALSE))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found that having ggplot2 (or plotly) plots as part of your dashboard, there is always a tiny bit of randomness present in the output. And hence the tests fail. It is better to explicitly export parts of the plot objects in my opinion, because they will be a more reliable yardstick to compare against. To do so, we add a few lines of code to &lt;code&gt;server.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exportTestValues(plot_balance = { ggplot_build(p_b)$data },
                 plot_total   = { ggplot_build(p_t)$data },
                 plot_import  = { ggplot_build(p_i)$data },
                 plot_export  = { ggplot_build(p_e)$data } )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a follow up, we customize which parts of the application‚Äôs state should be saved and checked for inside &lt;code&gt;app$snapshot()&lt;/code&gt;, using the &lt;code&gt;items =&lt;/code&gt; argument and update &lt;code&gt;dates.R&lt;/code&gt; so that only the &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;export&lt;/code&gt; (and &lt;em&gt;not the&lt;/em&gt; &lt;code&gt;output&lt;/code&gt;) sections of our .json files are evaluated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;...
app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot(items = list(input = TRUE, export = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is all you really need to get going with shinytest. Keep in mind that the package is still in development, and things might change in the future. For an in-depth walkthrough of shinytest‚Äôs capabilities, have a look at the &lt;a href=&#34;https://rstudio.github.io/shinytest/articles/shinytest.html&#34;&gt;official site&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-container-can-we-haz-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A üê≥ container, can we haz it?&lt;/h2&gt;
&lt;p&gt;Now that our shiny app is complete with test scripts, the whole thing can be packaged up and put inside a container. Of course we could deploy the shiny dashboard without a container too, but at the end of the day it makes everybody‚Äôs life a lot easier.&lt;/p&gt;
&lt;p&gt;Because if our container runs on our machine, it will also run on &lt;strong&gt;any machine&lt;/strong&gt; that has Docker. Without compatibility issues, independent from host version or platform distribution. In a real life scenario this significantly reduces time between prototypting and deployment, not the least because of the typically lightweight footprint of a Docker image.&lt;/p&gt;
&lt;p&gt;To containerize our shiny app, we first need to create an image that encompasses our:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Shiny application&lt;/li&gt;
&lt;li&gt;R packages our app needs&lt;/li&gt;
&lt;li&gt;System level dependencies these packages need&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We build our image layer by layer, starting with the &lt;a href=&#34;https://hub.docker.com/r/rocker/shiny/&#34;&gt;rocker/shiny image&lt;/a&gt; - which includes the minimal requirements for a Shiny Server. Then, we add everything else our application requires; finishing with copying the contents of our app to &lt;code&gt;/srv/shiny-server/usa-trade/&lt;/code&gt;, where the dashboard will be served from. These instructions are written to the &lt;code&gt;Dockerfile&lt;/code&gt;, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FROM rocker/shiny
MAINTAINER Tamas Szilagyi (tszilagyi@outlook.com)

## install R package dependencies (and clean up)
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2 \
    libssl-dev \
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## install packages from CRAN (and clean up)
RUN Rscript -e &amp;quot;install.packages(c(&amp;#39;devtools&amp;#39;,&amp;#39;dplyr&amp;#39;,&amp;#39;tidyr&amp;#39;,&amp;#39;fuzzyjoin&amp;#39;,&amp;#39;stringr&amp;#39;,&amp;#39;ggthemes&amp;#39;,&amp;#39;quantmod&amp;#39;,&amp;#39;ggplot2&amp;#39;,&amp;#39;shinydashboard&amp;#39;,&amp;#39;shinythemes&amp;#39;), repos=&amp;#39;https://cran.rstudio.com/&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install packages from github (and clean up)
RUN Rscript -e &amp;quot;devtools::install_github(&amp;#39;rstudio/shinytest&amp;#39;,&amp;#39;rstudio/webdriver&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install phantomjs
RUN Rscript -e &amp;quot;webdriver::install_phantomjs()&amp;quot;

## assume shiny app is in build folder /app2
COPY ./app2 /srv/shiny-server/usa-trade/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The smaller your Docker image, the better. Here‚Äôs a couple of guidelines to keep in mind when creating one:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Always use &lt;strong&gt;shared base images&lt;/strong&gt; (what comes after the &lt;code&gt;FROM&lt;/code&gt; statement) specific to your application, instead of trying to reinvent the wheel every time you write a Dockerfile.&lt;/li&gt;
&lt;li&gt;Try to &lt;strong&gt;avoid underused dependencies&lt;/strong&gt;. Going back to the my example app, I could‚Äôve installed the package &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyquant/index.html&#34;&gt;tidyquant&lt;/a&gt; to get my trade data in a tidy format out of the box, yet because the package has an insane amount of dependencies (including having Java installed); I wrote three &lt;a href=&#34;https://github.com/mtoto/markets_shiny/blob/master/app2/functions.R#L14&#34;&gt;helper functions&lt;/a&gt; instead.&lt;/li&gt;
&lt;li&gt;Make sure &lt;strong&gt;temporary files are removed&lt;/strong&gt; after the installation of libraries and packages.&lt;/li&gt;
&lt;li&gt;Push down commands that will likely invalidate the &lt;strong&gt;cache&lt;/strong&gt;, so Docker only rebuilds layers that change (more on this in the next section).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the Dockerfile finished, it is time to make ourselves familiar with the essential Docker commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;docker pull&lt;/strong&gt; pulls an image from the registry (Dockerhub).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker build&lt;/strong&gt; builds a docker image from our Dockerfile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run&lt;/strong&gt; instantiates the container from our image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker exec&lt;/strong&gt; execute commands from within the container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker rm&lt;/strong&gt; deletes a container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker login&lt;/strong&gt; login to Dockerhub (to upload our image).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker push&lt;/strong&gt; uploads the image back to Dockerhub.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let‚Äôs say we want to run our shiny app on a server that has Docker installed. Assuming we have a &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub repo&lt;/a&gt; containing all relevant files and our Dockerfile is to be found on &lt;a href=&#34;https://hub.docker.com/r/mtoto/shiny/&#34;&gt;Dockerhub&lt;/a&gt;, we can expose our shiny app to the world as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 clone into repo containing app 
git clone https://github.com/mtoto/markets_shiny.git
# 2 pull Docker file from Dockerhub
docker pull mtoto/shiny:latest
# 3 build Docker image, tag it &amp;#39;mtoto/shiny:latest&amp;#39;
docker build -t mtoto/shiny:latest .
# 4 run container in detached mode, listening on port 80, name it &amp;#39;site&amp;#39;
docker run -d -p 80:3838 --name site mtoto/shiny:latest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our app should be visible on &lt;em&gt;ht‚Äãps://myserver.com/usa-trade&lt;/em&gt; by default.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integration-with-travis-ci&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integration with Travis CI&lt;/h2&gt;
&lt;p&gt;If you are a seasoned R package developer, you are no stranger to Travis CI. It is a &lt;strong&gt;Continuous Integration&lt;/strong&gt; tool that automatically performs checks and runs tests on your code every time you push a commit to GitHub. The broad idea behind continuous integration is to encourage test-driven development, thereby allowing for frequent commits to the codebase without having to worry about integration problems.&lt;/p&gt;
&lt;p&gt;Travis supports many languages - including R, and can also build from Docker images. After creating an account on the &lt;a href=&#34;https://travis-ci.org&#34;&gt;Travis website&lt;/a&gt;, connect with GitHub and pick the repository for which you‚Äôd like to use it.&lt;/p&gt;
&lt;p&gt;The repo needs to contain a &lt;code&gt;.travis.yml&lt;/code&gt; file, encapsulating the instructions for Travis.
You‚Äôd tempted to write &lt;code&gt;language: R&lt;/code&gt; as the first line, but if we do that Travis will implicitly assume we are developing an R package and will start looking for the &lt;code&gt;DESCRIPTION&lt;/code&gt; file we do not have. Instead, I went with the undocumented option &lt;code&gt;language: generic&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, as we‚Äôll be only running Docker commands anyway.&lt;/p&gt;
&lt;p&gt;The naive approach would be to build our Docker image on every single run, instantiate a test container, run tests inside and upon success get rid of the container. Such a &lt;code&gt;.travis.yml&lt;/code&gt; would look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker build -t markets-shiny .
- docker run -d -p 3838:3838 markets-shiny:latest --name test

script:
- docker exec test R -f run_tests.R

after_script:
- docker rm -f test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem here is that we are building the Docker image from scratch with every single Travis run, resulting in a build time of over 20 minutes for my simple app. But our image is on Dockerhub, so why not pull it from there and take advantage of caching. Then, we‚Äôd only rebuild the changed layers after downloading the latest image from the registry.&lt;/p&gt;
&lt;p&gt;To make sure everything is nice and up to date, we will push the changes back to Dockerhub after every successful run. We need credentials to do so, but Travis conveniently allows for defining environment variables inside the repository settings (or via the CLI):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/gdTnLjd.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can go wild and revamp &lt;code&gt;.travis.yml&lt;/code&gt; accordingly:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker pull mtoto/shiny:latest
- docker build --cache-from mtoto/shiny:latest -t mtoto/shiny:latest . 
- docker run --rm -d -p 3838:3838 --name test mtoto/shiny:latest

script:
- docker exec test R -f /srv/shiny-server/usa-trade/run_tests.R

after_success:
- docker rm -f test
- docker login -u mtoto -p $DOCKER_PASSWORD
- docker tag mtoto/shiny:latest mtoto/shiny:$TRAVIS_BUILD_NUMBER
- docker push mtoto/shiny&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the second run (once the latest image is on Dockerhub), the build time is reduced by a factor of 10. Sweet. When we use the flag &lt;code&gt;--cache-from&lt;/code&gt;, Docker only rebuilds changed layers, ie. modifications to our shiny app. We can see this in the Travis log as &lt;code&gt;---&amp;gt; Using cache&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/hTNuQhY.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Keep in mind when making significant changes to your dashboard, it is important to update the tests that create fresh expected outputs reflecting these changes. If you don‚Äôt trust the outputs will align, remember to use &lt;code&gt;exportTestValues()&lt;/code&gt; and fill it up with the new objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;was-it-all-worth-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Was it all worth it?&lt;/h2&gt;
&lt;p&gt;While this workflow might feel like over-engineering, once all the tools are set up to work in tandem, shiny dashboard development becomes surprisingly efficient. The icing on the cake is that you are creating a dashboard that is pretty much ready for deployment from day one. Devops will love you for it, trust me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In reality this points to &lt;code&gt;language: bash&lt;/code&gt;, &lt;code&gt;language: sh&lt;/code&gt; and &lt;code&gt;language: shell&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>